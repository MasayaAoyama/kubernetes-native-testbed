# Generated by below command
#   helm2 repo add pingcap https://charts.pingcap.org/
#   helm2 repo update
#   helm2 fetch --version v1.0.6 pingcap/tidb-cluster
#   helm2 template --name cart-db --namespace cart --set tidb.replicas=0 --set pd.storageClassName=rook-ceph-block --set tikv.storageClassName=rook-ceph-block tidb-cluster-v1.0.6.tgz > db.yaml
#   # add namespace cart to each metadata
---
# Source: tidb-cluster/templates/monitor-secret.yaml

apiVersion: v1
kind: Secret
metadata:
  name: cart-db-monitor
  namespace: cart
  labels:
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: monitor
    helm.sh/chart: tidb-cluster-v1.0.6
type: Opaque
data:
  username: YWRtaW4=
  password: YWRtaW4=

---
# Source: tidb-cluster/templates/monitor-configmap.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: cart-db-monitor
  namespace: cart
  labels:
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: monitor
    helm.sh/chart: tidb-cluster-v1.0.6
data:
  prometheus-config: |-
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    scrape_configs:
      - job_name: 'tidb-cluster'
        scrape_interval: 15s
        honor_labels: true
        kubernetes_sd_configs:
        - role: pod
          namespaces:
            names:
            - cart
        tls_config:
          insecure_skip_verify: true
        relabel_configs:
        - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_instance]
          action: keep
          regex: cart-db
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_pod_node_name]
          action: replace
          target_label: kubernetes_node
        - source_labels: [__meta_kubernetes_pod_ip]
          action: replace
          target_label: kubernetes_pod_ip
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: instance
        - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_instance]
          action: replace
          target_label: cluster
    rule_files:
      - '/prometheus-rules/rules/*.rules.yml'
    
  dashboard-config: |-
    {
        "apiVersion": 1,
        "providers": [
            {
                "folder": "",
                "name": "0",
                "options": {
                    "path": "/grafana-dashboard-definitions/tidb"
                },
                "orgId": 1,
                "type": "file"
            }
        ]
    }
    

---
# Source: tidb-cluster/templates/pd-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cart-db-pd-cfa0d77a
  namespace: cart
  labels:
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: pd
    helm.sh/chart: tidb-cluster-v1.0.6
data:
  startup-script: |-
    #!/bin/sh
    
    # This script is used to start pd containers in kubernetes cluster
    
    # Use DownwardAPIVolumeFiles to store informations of the cluster:
    # https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/#the-downward-api
    #
    #   runmode="normal/debug"
    #
    
    set -uo pipefail
    
    
    
    ANNOTATIONS="/etc/podinfo/annotations"
    
    if [[ ! -f "${ANNOTATIONS}" ]]
    then
        echo "${ANNOTATIONS} does't exist, exiting."
        exit 1
    fi
    source ${ANNOTATIONS} 2>/dev/null
    
    runmode=${runmode:-normal}
    if [[ X${runmode} == Xdebug ]]
    then
        echo "entering debug mode."
        tail -f /dev/null
    fi
    
    # Use HOSTNAME if POD_NAME is unset for backward compatibility.
    POD_NAME=${POD_NAME:-$HOSTNAME}
    # the general form of variable PEER_SERVICE_NAME is: "<clusterName>-pd-peer"
    cluster_name=`echo ${PEER_SERVICE_NAME} | sed 's/-pd-peer//'`
    domain="${POD_NAME}.${PEER_SERVICE_NAME}.${NAMESPACE}.svc"
    discovery_url="${cluster_name}-discovery.${NAMESPACE}.svc:10261"
    encoded_domain_url=`echo ${domain}:2380 | base64 | tr "\n" " " | sed "s/ //g"`
    
    elapseTime=0
    period=1
    threshold=30
    while true; do
        sleep ${period}
        elapseTime=$(( elapseTime+period ))
    
        if [[ ${elapseTime} -ge ${threshold} ]]
        then
            echo "waiting for pd cluster ready timeout" >&2
            exit 1
        fi
    
        if nslookup ${domain} 2>/dev/null
        then
            echo "nslookup domain ${domain}.svc success"
            break
        else
            echo "nslookup domain ${domain} failed" >&2
        fi
    done
    
    ARGS="--data-dir=/var/lib/pd \
    --name=${POD_NAME} \
    --peer-urls=http://0.0.0.0:2380 \
    --advertise-peer-urls=http://${domain}:2380 \
    --client-urls=http://0.0.0.0:2379 \
    --advertise-client-urls=http://${domain}:2379 \
    --config=/etc/pd/pd.toml \
    "
    
    if [[ -f /var/lib/pd/join ]]
    then
        # The content of the join file is:
        #   demo-pd-0=http://demo-pd-0.demo-pd-peer.demo.svc:2380,demo-pd-1=http://demo-pd-1.demo-pd-peer.demo.svc:2380
        # The --join args must be:
        #   --join=http://demo-pd-0.demo-pd-peer.demo.svc:2380,http://demo-pd-1.demo-pd-peer.demo.svc:2380
        join=`cat /var/lib/pd/join | tr "," "\n" | awk -F'=' '{print $2}' | tr "\n" ","`
        join=${join%,}
        ARGS="${ARGS} --join=${join}"
    elif [[ ! -d /var/lib/pd/member/wal ]]
    then
        until result=$(wget -qO- -T 3 http://${discovery_url}/new/${encoded_domain_url} 2>/dev/null); do
            echo "waiting for discovery service to return start args ..."
            sleep $((RANDOM % 5))
        done
        ARGS="${ARGS}${result}"
    fi
    
    echo "starting pd-server ..."
    sleep $((RANDOM % 10))
    echo "/pd-server ${ARGS}"
    exec /pd-server ${ARGS}
    
  config-file: |-
    [log]
    level = "info"
    [replication]
    location-labels = ["region", "zone", "rack", "host"]
    

---
# Source: tidb-cluster/templates/tidb-configmap-rollout.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: cart-db-tidb-a4c4bb14
  namespace: cart
  labels:
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: tidb
    helm.sh/chart: tidb-cluster-v1.0.6
data:
  startup-script: |-
    #!/bin/sh
    
    # This script is used to start tidb containers in kubernetes cluster
    
    # Use DownwardAPIVolumeFiles to store informations of the cluster:
    # https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/#the-downward-api
    #
    #   runmode="normal/debug"
    #
    set -uo pipefail
    
    
    
    ANNOTATIONS="/etc/podinfo/annotations"
    
    if [[ ! -f "${ANNOTATIONS}" ]]
    then
        echo "${ANNOTATIONS} does't exist, exiting."
        exit 1
    fi
    source ${ANNOTATIONS} 2>/dev/null
    runmode=${runmode:-normal}
    if [[ X${runmode} == Xdebug ]]
    then
        echo "entering debug mode."
        tail -f /dev/null
    fi
    
    ARGS="--store=tikv \
    --host=0.0.0.0 \
    --path=${CLUSTER_NAME}-pd:2379 \
    --config=/etc/tidb/tidb.toml
    "
    
    if [[ X${BINLOG_ENABLED:-} == Xtrue ]]
    then
        ARGS="${ARGS} --enable-binlog=true"
    fi
    
    SLOW_LOG_FILE=${SLOW_LOG_FILE:-""}
    if [[ ! -z "${SLOW_LOG_FILE}" ]]
    then
        ARGS="${ARGS} --log-slow-query=${SLOW_LOG_FILE:-}"
    fi
    
    echo "start tidb-server ..."
    echo "/tidb-server ${ARGS}"
    exec /tidb-server ${ARGS}
    
  config-file: |-
    [log]
    level = "info"
    

---
# Source: tidb-cluster/templates/tidb-configmap.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: cart-db-tidb
  namespace: cart
  labels:
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: tidb
    helm.sh/chart: tidb-cluster-v1.0.6
data:
  startup-script: |-
    #!/bin/sh
    
    # This script is used to start tidb containers in kubernetes cluster
    
    # Use DownwardAPIVolumeFiles to store informations of the cluster:
    # https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/#the-downward-api
    #
    #   runmode="normal/debug"
    #
    set -uo pipefail
    
    
    
    ANNOTATIONS="/etc/podinfo/annotations"
    
    if [[ ! -f "${ANNOTATIONS}" ]]
    then
        echo "${ANNOTATIONS} does't exist, exiting."
        exit 1
    fi
    source ${ANNOTATIONS} 2>/dev/null
    runmode=${runmode:-normal}
    if [[ X${runmode} == Xdebug ]]
    then
        echo "entering debug mode."
        tail -f /dev/null
    fi
    
    ARGS="--store=tikv \
    --host=0.0.0.0 \
    --path=${CLUSTER_NAME}-pd:2379 \
    --config=/etc/tidb/tidb.toml
    "
    
    if [[ X${BINLOG_ENABLED:-} == Xtrue ]]
    then
        ARGS="${ARGS} --enable-binlog=true"
    fi
    
    SLOW_LOG_FILE=${SLOW_LOG_FILE:-""}
    if [[ ! -z "${SLOW_LOG_FILE}" ]]
    then
        ARGS="${ARGS} --log-slow-query=${SLOW_LOG_FILE:-}"
    fi
    
    echo "start tidb-server ..."
    echo "/tidb-server ${ARGS}"
    exec /tidb-server ${ARGS}
    
  config-file: |-
    [log]
    level = "info"
    

---
# Source: tidb-cluster/templates/tikv-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cart-db-tikv-3a588779
  namespace: cart
  labels:
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: tikv
    helm.sh/chart: tidb-cluster-v1.0.6
data:
  startup-script: |-
    #!/bin/sh
    
    # This script is used to start tikv containers in kubernetes cluster
    
    # Use DownwardAPIVolumeFiles to store informations of the cluster:
    # https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/#the-downward-api
    #
    #   runmode="normal/debug"
    #
    
    set -uo pipefail
    
    
    
    ANNOTATIONS="/etc/podinfo/annotations"
    
    if [[ ! -f "${ANNOTATIONS}" ]]
    then
        echo "${ANNOTATIONS} does't exist, exiting."
        exit 1
    fi
    source ${ANNOTATIONS} 2>/dev/null
    
    runmode=${runmode:-normal}
    if [[ X${runmode} == Xdebug ]]
    then
    	echo "entering debug mode."
    	tail -f /dev/null
    fi
    
    # Use HOSTNAME if POD_NAME is unset for backward compatibility.
    POD_NAME=${POD_NAME:-$HOSTNAME}
    ARGS="--pd=${CLUSTER_NAME}-pd:2379 \
    --advertise-addr=${POD_NAME}.${HEADLESS_SERVICE_NAME}.${NAMESPACE}.svc:20160 \
    --addr=0.0.0.0:20160 \
    --status-addr=0.0.0.0:20180 \
    --data-dir=/var/lib/tikv \
    --capacity=${CAPACITY} \
    --config=/etc/tikv/tikv.toml
    "
    
    echo "starting tikv-server ..."
    echo "/tikv-server ${ARGS}"
    exec /tikv-server ${ARGS}
    
  config-file: |-
    log-level = "info"
    

---
# Source: tidb-cluster/templates/discovery-rbac.yaml

kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: cart-db-discovery
  namespace: cart
  labels:
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: discovery
    helm.sh/chart: tidb-cluster-v1.0.6
rules:
- apiGroups: ["pingcap.com"]
  resources: ["tidbclusters"]
  resourceNames: [cart-db]
  verbs: ["get"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: cart-db-discovery
  namespace: cart
  labels:
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: discovery
    helm.sh/chart: tidb-cluster-v1.0.6
subjects:
- kind: ServiceAccount
  name: cart-db-discovery
roleRef:
  kind: Role
  name: cart-db-discovery
  apiGroup: rbac.authorization.k8s.io
---
kind: ServiceAccount
apiVersion: v1
metadata:
  name: cart-db-discovery
  namespace: cart
  labels:
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: discovery
    helm.sh/chart: tidb-cluster-v1.0.6

---
# Source: tidb-cluster/templates/monitor-rbac.yaml

kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: cart-db-monitor
  namespace: cart
  labels:
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: monitor
    helm.sh/chart: tidb-cluster-v1.0.6
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: cart-db-monitor
  namespace: cart
  labels:
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: monitor
    helm.sh/chart: tidb-cluster-v1.0.6
subjects:
- kind: ServiceAccount
  name: cart-db-monitor
roleRef:
  kind: Role
  name: cart-db-monitor
  apiGroup: rbac.authorization.k8s.io
---
kind: ServiceAccount
apiVersion: v1
metadata:
  name: cart-db-monitor
  namespace: cart
  labels:
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: monitor
    helm.sh/chart: tidb-cluster-v1.0.6

---
# Source: tidb-cluster/templates/discovery-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: cart-db-discovery
  namespace: cart
  labels:
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: discovery
    helm.sh/chart: tidb-cluster-v1.0.6
spec:
  type: ClusterIP
  ports:
  - name: discovery
    port: 10261
    targetPort: 10261
    protocol: TCP
  selector:
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: discovery

---
# Source: tidb-cluster/templates/monitor-service.yaml

apiVersion: v1
kind: Service
metadata:
  name: cart-db-grafana
  namespace: cart
  labels:
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: monitor
    helm.sh/chart: tidb-cluster-v1.0.6
spec:
  ports:
  - name: grafana
    port: 3000
    protocol: TCP
    targetPort: 3000
  type: NodePort
  selector:
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: monitor
---
apiVersion: v1
kind: Service
metadata:
  name: cart-db-monitor-reloader
  namespace: cart
  labels:
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: monitor
    helm.sh/chart: tidb-cluster-v1.0.6
spec:
  ports:
  - name: reloader
    port: 9089
    protocol: TCP
    targetPort: 9089
  type: NodePort
  selector:
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: monitor
---
apiVersion: v1
kind: Service
metadata:
  name: cart-db-prometheus
  namespace: cart
  labels:
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: monitor
    helm.sh/chart: tidb-cluster-v1.0.6
spec:
  ports:
  - name: prometheus
    port: 9090
    protocol: TCP
    targetPort: 9090
  type: NodePort
  selector:
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: monitor

---
# Source: tidb-cluster/templates/tidb-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: cart-db-tidb
  namespace: cart
  labels:
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: tidb
    helm.sh/chart: tidb-cluster-v1.0.6
spec:
  type: NodePort
  ports:
  - name: mysql-client
    port: 4000
    targetPort: 4000
    protocol: TCP
  - name: status
    port: 10080
    targetPort: 10080
    protocol: TCP
  selector:
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: tidb

---
# Source: tidb-cluster/templates/discovery-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cart-db-discovery
  namespace: cart
  labels:
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: discovery
    helm.sh/chart: tidb-cluster-v1.0.6
spec:
  # don't modify this replicas
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: tidb-cluster
      app.kubernetes.io/instance: cart-db
      app.kubernetes.io/component: discovery
  template:
    metadata:
      labels:
        app.kubernetes.io/name: tidb-cluster
        app.kubernetes.io/instance: cart-db
        app.kubernetes.io/component: discovery
    spec:
      serviceAccount: cart-db-discovery
      containers:
      - name: discovery
        image: pingcap/tidb-operator:v1.0.6
        imagePullPolicy: IfNotPresent
        resources:
            limits:
              cpu: 250m
              memory: 150Mi
            requests:
              cpu: 80m
              memory: 50Mi
            
        command:
          - /usr/local/bin/tidb-discovery
        env:
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace

---
# Source: tidb-cluster/templates/monitor-deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: cart-db-monitor
  namespace: cart
  labels:
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: monitor
    helm.sh/chart: tidb-cluster-v1.0.6
spec:
  replicas: 1
  strategy:
    type: Recreate
    rollingUpdate: null
  selector:
    matchLabels:
      app.kubernetes.io/name: tidb-cluster
      app.kubernetes.io/instance: cart-db
      app.kubernetes.io/component: monitor
  template:
    metadata:
      labels:
        app.kubernetes.io/name: tidb-cluster
        app.kubernetes.io/instance: cart-db
        app.kubernetes.io/component: monitor
    spec:
      serviceAccount: cart-db-monitor
      initContainers:
      - name: monitor-initializer
        image: pingcap/tidb-monitor-initializer:v3.0.5
        imagePullPolicy: Always
        env:
        - name: GF_PROVISIONING_PATH
          value: /grafana-dashboard-definitions/tidb
        - name: GF_DATASOURCE_PATH
          value: /etc/grafana/provisioning/datasources
        - name: TIDB_CLUSTER_NAME
          value: cart-db
        - name: TIDB_ENABLE_BINLOG
          value: "false"
        - name:  PROM_CONFIG_PATH
          value: /prometheus-rules
        - name: PROM_PERSISTENT_DIR
          value: /data
        - name: TIDB_VERSION
          value: pingcap/tidb:v3.0.5
        - name: GF_K8S_PROMETHEUS_URL
          value: http://prometheus-k8s.monitoring.svc:9090
        - name: GF_TIDB_PROMETHEUS_URL
          value: http://127.0.0.1:9090
        - name: TIDB_CLUSTER_NAMESPACE
          value: cart
        command:
        - /bin/sh
        - -c
        - |
          mkdir -p /data/prometheus /data/grafana
          chmod 777 /data/prometheus /data/grafana
          /usr/bin/init.sh
        securityContext:
          runAsUser: 0
        volumeMounts:
        - mountPath: /grafana-dashboard-definitions/tidb
          name: grafana-dashboard
          readOnly: false
        - name: prometheus-rules
          mountPath: /prometheus-rules
          readOnly: false
        - name: monitor-data
          mountPath: /data
        - name: datasource
          mountPath: /etc/grafana/provisioning/datasources
          readOnly: false
        resources:
          {}
          
      containers:
      - name: prometheus
        image: prom/prometheus:v2.11.1
        imagePullPolicy: IfNotPresent
        resources:
            limits: {}
            requests: {}
            
        command:
        - /bin/prometheus
        - --web.enable-admin-api
        - --web.enable-lifecycle
        - --log.level=info
        - --config.file=/etc/prometheus/prometheus.yml
        - --storage.tsdb.path=/data/prometheus
        - --storage.tsdb.retention=12d
        ports:
        - name: prometheus
          containerPort: 9090
          protocol: TCP
        # `TZ` is unused in Prometheus Docker image, we set it here just to keep consistency
        env:
        - name: TZ
          value: UTC
        volumeMounts:
          - name: prometheus-config
            mountPath: /etc/prometheus
            readOnly: true
          - name: monitor-data
            mountPath: /data
          - name: prometheus-rules
            mountPath: /prometheus-rules
            readOnly: false
      - name: reloader
        image: pingcap/tidb-monitor-reloader:v1.0.1
        imagePullPolicy: IfNotPresent
        command:
        - /bin/reload
        - --root-store-path=/data
        - --sub-store-path=pingcap/tidb:v3.0.5
        - --watch-path=/prometheus-rules/rules
        - --prometheus-url=http://127.0.0.1:9090
        ports:
        - name: reloader
          containerPort: 9089
          protocol: TCP
        volumeMounts:
          - name: prometheus-rules
            mountPath: /prometheus-rules
            readOnly: false
          - name: monitor-data
            mountPath: /data
        resources:
          null
          
      - name: grafana
        image: grafana/grafana:6.0.1
        imagePullPolicy: IfNotPresent
        resources:
            limits: {}
            requests: {}
            
        ports:
        - name: grafana
          containerPort: 3000
          protocol: TCP
        env:
        - name: GF_PATHS_DATA
          value: /data/grafana
        - name: GF_SECURITY_ADMIN_USER
          valueFrom:
            secretKeyRef:
              name: cart-db-monitor
              key: username
        - name: GF_SECURITY_ADMIN_PASSWORD
          valueFrom:
            secretKeyRef:
              name: cart-db-monitor
              key: password
        - name: GF_AUTH_ANONYMOUS_ENABLED
          value: "true"
        - name: GF_AUTH_ANONYMOUS_ORG_NAME
          value: "Main Org."
        - name: GF_AUTH_ANONYMOUS_ORG_ROLE
          value: "Viewer"
        - name: TZ
          value: UTC
        volumeMounts:
          - name: monitor-data
            mountPath: /data
          - mountPath: /etc/grafana/provisioning/datasources
            name: datasource
            readOnly: false
          - mountPath: /etc/grafana/provisioning/dashboards
            name: dashboards-provisioning
            readOnly: false
          - mountPath: /grafana-dashboard-definitions/tidb
            name: grafana-dashboard
            readOnly: false
      volumes:
      - name: monitor-data
        emptyDir: {}
      - name: prometheus-config
        configMap:
          name: cart-db-monitor
          items:
          - key: prometheus-config
            path: prometheus.yml
      - emptyDir: {}
        name: datasource
      - configMap:
          name: cart-db-monitor
          items:
          - key: dashboard-config
            path: dashboards.yaml
        name: dashboards-provisioning
      - emptyDir: {}
        name: prometheus-rules
      - emptyDir: {}
        name: grafana-dashboard

---
# Source: tidb-cluster/templates/tidb-cluster.yaml
apiVersion: pingcap.com/v1alpha1
kind: TidbCluster
metadata:
  name: cart-db
  namespace: cart
  annotations:
    pingcap.com/pd.cart-db-pd.sha: "cfa0d77a"
    pingcap.com/tikv.cart-db-tikv.sha: "3a588779"
    pingcap.com/tidb.cart-db-tidb.sha: "a4c4bb14"
  labels:
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: tidb-cluster
    helm.sh/chart: tidb-cluster-v1.0.6
spec:
  pvReclaimPolicy: Retain
  timezone: UTC
  services:
    - name: pd
      type: ClusterIP
    
  schedulerName: tidb-scheduler
  pd:
    replicas: 3
    image: pingcap/pd:v3.0.5
    imagePullPolicy: IfNotPresent
    storageClassName: rook-ceph-block
    limits: {}
    requests:
      storage: 1Gi
    
    affinity:
      {}
      
    nodeSelector:
      {}
      
    hostNetwork: false
  tikv:
    replicas: 3
    image: pingcap/tikv:v3.0.5
    imagePullPolicy: IfNotPresent
    storageClassName: rook-ceph-block
    limits: {}
    requests:
      storage: 10Gi
    
    affinity:
      {}
      
    nodeSelector:
      {}
      
    maxFailoverCount: 3
    hostNetwork: false
  tidb:
    replicas: 0
    image: pingcap/tidb:v3.0.5
    imagePullPolicy: IfNotPresent
    limits: {}
    requests: {}
    
    affinity:
      {}
      
    nodeSelector:
      {}
      
    hostNetwork: false
    binlogEnabled: false
    maxFailoverCount: 3
    separateSlowLog: true
    slowLogTailer:
      image: busybox:1.26.2
      imagePullPolicy: IfNotPresent
      limits:
        cpu: 100m
        memory: 50Mi
      requests:
        cpu: 20m
        memory: 5Mi
      

---
# Source: tidb-cluster/templates/drainer-configmap-rollout.yaml

---
# Source: tidb-cluster/templates/drainer-configmap.yaml

---
# Source: tidb-cluster/templates/drainer-service.yaml


---
# Source: tidb-cluster/templates/drainer-statefulset.yaml


---
# Source: tidb-cluster/templates/monitor-pvc.yaml


---
# Source: tidb-cluster/templates/pump-configmap-rollout.yaml


---
# Source: tidb-cluster/templates/pump-configmap.yaml


---
# Source: tidb-cluster/templates/pump-service.yaml


---
# Source: tidb-cluster/templates/pump-statefulset.yaml


---
# Source: tidb-cluster/templates/scheduled-backup-cronjob.yaml


---
# Source: tidb-cluster/templates/scheduled-backup-pvc.yaml


---
# Source: tidb-cluster/templates/tidb-initializer-job.yaml


---
# Source: tidb-cluster/templates/tikv-importer-configmap.yaml


---
# Source: tidb-cluster/templates/tikv-importer-service.yaml


---
# Source: tidb-cluster/templates/tikv-importer-statefulset.yaml


